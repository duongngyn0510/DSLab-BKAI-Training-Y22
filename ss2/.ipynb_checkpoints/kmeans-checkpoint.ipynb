{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member:\n",
    "    def __init__(self, r_d, label=None, doc_id=None):\n",
    "        self.r_d = r_d\n",
    "        self.label = label\n",
    "        self.doc_id = doc_id\n",
    "    \n",
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self.centroid = None\n",
    "        self.members = []\n",
    "\n",
    "    def reset_members(self):\n",
    "        self.members = []\n",
    "    \n",
    "    def add_members(self, member):\n",
    "        self.members.append(member)\n",
    "        \n",
    "class Kmeans:\n",
    "    def __init__(self, num_clusters):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.clusters = [Cluster() for _ in range(self.num_clusters)]\n",
    "        self.E = []     \n",
    "        self.S = 0 \n",
    "        \n",
    "    def load_data(self, data_path):\n",
    "        def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "            r_d = [0.0 for _ in range(vocab_size)]\n",
    "            indices_tfidfs = sparse_r_d.split()\n",
    "            for index_tfidf in indices_tfidfs:\n",
    "                index = int(index_tfidf.split(':')[0])\n",
    "                tfidf = float(index_tfidf.split(':')[1])\n",
    "                r_d[index] = tfidf\n",
    "            return np.array(r_d)\n",
    "        \n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        with open(data_path + '/../' + 'word_idfs.txt') as f:\n",
    "            vocab_size = len(f.read().splitlines()\n",
    "        self.data = []\n",
    "        self.label_count = defaultdict(int)\n",
    "        for data_id, d in enumerate(d_lines):\n",
    "            feature = d.split('<fff>')\n",
    "            label, doc_id = int(feature[0]), int(feature[1])\n",
    "            self.label_count[label] += 1\n",
    "            r_d = sparse_to_dense(sparse_r_d=feature[2], vocab_size=vocab_size)\n",
    "\n",
    "            self.data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
    "\n",
    "    def random_init(self, seed_value=\"random\"):    \n",
    "        if seed_value == \"kmeans++\":\n",
    "            pass\n",
    "        else:                               \n",
    "            rand_centroids = np.random.choice(self.data, size=self.num_clusters, replace=False)         \n",
    "            \n",
    "            for cluster_idx in range(self.num_clusters):\n",
    "                self.clusters[cluster_idx].centroid = rand_centroids[cluster_idx].r_d\n",
    "            \n",
    "    def compute_similarity(self, member, centroid):\n",
    "        return np.dot(member.r_d, centroid) /           \\\n",
    "                (np.linalg.norm(member.r_d, ord=2)*np.linalg.norm(centroid, ord=2))\n",
    "\n",
    "    def select_clusters_for(self, member):\n",
    "        best_fit_cluster = None\n",
    "        max_similarity = -1\n",
    "        for cluster in self.clusters:\n",
    "            similarity = self.compute_similarity(member, cluster.centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = similarity\n",
    "        \n",
    "        best_fit_cluster.add_members(member)\n",
    "        return max_similarity\n",
    "\n",
    "    def update_centroid_of(self, cluster):\n",
    "        member_r_ds = [member.r_d for member in cluster.members]\n",
    "        aver_r_d = np.mean(member_r_ds, axis=0)\n",
    "        # sqrt_sum_sqr = np.sqrt(np.sum(aver_r_d**2))\n",
    "        # new_centroid = np.array([value/sqrt_sum_sqr for value in aver_r_d])       \n",
    "                                                                                    \n",
    "        # cluster.centroid = new_centroid                                                           \n",
    "        cluster.centroid = aver_r_d\n",
    "\n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        criteria = ['centroid', 'similarity', 'max_iters']\n",
    "        assert criterion in criteria\n",
    "        if criterion == 'max_iters':\n",
    "            if self.iteration >= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif criterion == 'centroid':\n",
    "            E_new = [list(cluster.centroid) for cluster in self.clusters]\n",
    "            E_new_minus_E = [centroid for centroid in E_new if centroid not in self.E]\n",
    "            self.E = E_new\n",
    "            if len(E_new_minus_E) <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            new_S_minus_S = self.new_S - self.S\n",
    "            self.S = self.new_S\n",
    "            if new_S_minus_S <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def run(self, seed_value, criterion, threshold):\n",
    "        self.random_init(seed_value)\n",
    "\n",
    "        self.iteration = 0\n",
    "        while True:\n",
    "            for cluster in self.clusters:\n",
    "                cluster.reset_members()\n",
    "            self.new_S = 0\n",
    "            for member in self.data:\n",
    "                max_S = self.select_clusters_for(member)\n",
    "                self.new_S += max_S\n",
    "            for cluster in self.clusters:\n",
    "                self.update_centroid_of(cluster)\n",
    "            \n",
    "            self.iteration += 1\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                break\n",
    "\n",
    "\n",
    "    def compute_purity(self):\n",
    "        majority_sum = 0\n",
    "        for cluster in self.clusters:\n",
    "            member_labels = [member.label for member in cluster.members]\n",
    "            max_count = max([member_labels.count(label) for label in range(self.num_clusters)])\n",
    "            majority_sum += max_count\n",
    "\n",
    "        return majority_sum * 1./len(self.data)\n",
    "\n",
    "    def compute_NMI(self):\n",
    "        I_value, H_omega, H_C, N = 0., 0., 0., len(self.data)\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            wk = len(cluster.members) * 1.\n",
    "            H_omega +=  -wk/N * np.log10(wk/N)\n",
    "            member_labels = [member.label for member in cluster.members]\n",
    "            for label in range(20):\n",
    "                wk_cj = member_labels.count(label)*1.\n",
    "                cj = self.label_count[label]\n",
    "                I_value += wk_cj/N * np.log10(N * wk_cj/(wk*cj) + 1e-12) #what 1e-12 for?\n",
    "        \n",
    "        for label in range(20):\n",
    "            cj = self.label_count[label] * 1.\n",
    "            H_C += -cj/N * np.log10(cj/N)\n",
    "\n",
    "        return I_value * 2 / (H_omega + H_C)\n",
    "\n",
    "def load_data(data_path):                       \n",
    "    def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "        r_d = [0.0 for _ in range(vocab_size)]\n",
    "        indices_tfidfs = sparse_r_d.split()\n",
    "        for index_tfidf in indices_tfidfs:\n",
    "            index = int(index_tfidf.split(':')[0])\n",
    "            tfidf = float(index_tfidf.split(':')[1])\n",
    "            r_d[index] = tfidf\n",
    "        return np.array(r_d)\n",
    "\n",
    "    with open(data_path) as f:\n",
    "        d_lines = f.read().splitlines()\n",
    "    with open(data_path + '/../' + 'word_idfs.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "        \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for data_id, d in enumerate(d_lines):\n",
    "        feature = d.split('<fff>')\n",
    "        label, doc_id = int(feature[0]), int(feature[1])\n",
    "        labels.append(label)\n",
    "        r_d = sparse_to_dense(sparse_r_d=feature[2], vocab_size=vocab_size)\n",
    "\n",
    "        data.append(r_d)\n",
    "\n",
    "    return data, labels     \n",
    "\n",
    "def compute_accuracy(predicted_y, expected_y):                              \n",
    "    matches = np.equal(predicted_y, expected_y)\n",
    "    accuracy = np.sum(matches.astype(float)) / len(expected_y)              \n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def clustering_with_KMeans(data_path): \n",
    "    data, labels = load_data(data_path)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    from scipy.sparse import csr_matrix\n",
    "    X = csr_matrix(data)\n",
    "    print('=========')\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=20,\n",
    "        init='random',\n",
    "        n_init=5,\n",
    "        tol=1e-3,\n",
    "        random_state=2018\n",
    "    ).fit(X)\n",
    "    \n",
    "    label = kmeans.labels_\n",
    "  \n",
    "\n",
    "def classifying_with_linear_SVMs(data_path_1, data_path_2):\n",
    "    train_X, train_y = load_data(data_path_1)               \n",
    "    from sklearn.svm import LinearSVC\n",
    "    classifier = LinearSVC(\n",
    "        C=10.0,\n",
    "        tol=0.001,\n",
    "        verbose=True\n",
    "    ) \n",
    "    classifier.fit(train_X, train_y)\n",
    "\n",
    "    test_X, test_y = load_data(data_path_2)             \n",
    "    predict_y = classifier.predict(test_X)\n",
    "    accuracy = compute_accuracy(predicted_y=predict_y, expected_y=test_y)\n",
    "    print('Accuracy =', accuracy)\n",
    "\n",
    "news_groups = Kmeans(20)\n",
    "news_groups.load_data('../20news-bydate/20news-full-tfidf.txt')\n",
    "news_groups.run(seed_value=\"random\", criterion=\"max_iters\", threshold=20)           \n",
    "accuracy = news_groups.compute_purity()\n",
    "print(accuracy)\n",
    "\n",
    "clustering_with_KMeans('../20news-bydate/20news-full-tfidf.txt')\n",
    "\n",
    "classifying_with_linear_SVMs('../20news-bydate/20news-train-tfidf.txt', \\\n",
    "                            '../20news-bydate/20news-test-tfidf.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
